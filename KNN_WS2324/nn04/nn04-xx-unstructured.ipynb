{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMthmpz3STuvFd71UAo1MzP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/schmidt-marvin/ESI_2022_TecAA/blob/main/KNN_WS2324/nn04/nn04-xx-unstructured.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Einfaches Beispiel RNN\n",
        "Das erste Beispiel zeigt eine einfache Implementierung eines RNN, das lediglich eine verrauschte Sinusfunktion approximieren soll"
      ],
      "metadata": {
        "id": "Bn1tKgnF0tUP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFnDb3Xy0SuA"
      },
      "outputs": [],
      "source": [
        "#simple expample: approximation of sin+rand by RNN\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "#from tensorflow.data import Dataset\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "#generate data\n",
        "v1=np.sin(np.linspace(0,4000,4000*5))+0.4*np.sin(4*np.linspace(0,4000,4000*5))\n",
        "v1=v1+0.6*np.random.randn(len(v1))\n",
        "v1=v1/np.max(v1)\n",
        "v2=v1\n",
        "\n",
        "#init parameter\n",
        "train_split=int(len(v1)/2) #how to split training and validation (no test data here)\n",
        "samp_size=20 #length of a sequence the RNN is supposed to process\n",
        "\n",
        "batch_size = 300\n",
        "epochs=8\n",
        "buffer_size = 100\n",
        "\n",
        "#Helper function reshape data\n",
        "def univariate_data(dataset, target_data, start_index, end_index, history_size, target_size):\n",
        "  data = []\n",
        "  labels = []\n",
        "\n",
        "  start_index = start_index + history_size\n",
        "  if end_index is None:\n",
        "    end_index = len(dataset) - target_size\n",
        "\n",
        "  for i in range(start_index, end_index):\n",
        "    indices = range(i-history_size, i)\n",
        "    # Reshape data from (history_size,) to (history_size, 1)\n",
        "    data.append(np.reshape(dataset[indices], (history_size, 1)))\n",
        "    labels.append(target_data[i+target_size])\n",
        "  return np.array(data), np.array(labels)\n",
        "\n",
        "\n",
        "x_train, y_train= univariate_data(v1,v2, 0, train_split,\n",
        "                                           samp_size,\n",
        "                                           5)\n",
        "x_val, y_val= univariate_data(v1,v2, train_split,None,\n",
        "                                       samp_size,\n",
        "                                       5)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print ('First signal')\n",
        "print (x_train[0])\n",
        "print ('\\n First Value to predict')\n",
        "print (y_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#use keras tool to prepare dataset for training and validation\n",
        "train_univariate = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_univariate = train_univariate.cache().shuffle(buffer_size).batch(batch_size).repeat()\n",
        "\n",
        "val_univariate = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "val_univariate = val_univariate.batch(batch_size).repeat()"
      ],
      "metadata": {
        "id": "rIRCekrP004y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install numpy==1.19 (big nope m8)"
      ],
      "metadata": {
        "id": "pObknbdu03ZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build RNN model by means of keras\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(layers.SimpleRNN(20,activation='tanh',input_shape=x_train.shape[-2:])) #A_1 in upper diagram #,input_shape=x_train.shape[-2:]\n",
        "model.add(layers.Dense(1,activation='tanh')) #A_2 in the upper diagram\n",
        "\n",
        "# setup optimizer and define loss\n",
        "opt=optimizers.SGD(learning_rate=0.001)\n",
        "model.build()\n",
        "model.compile(loss='mae', optimizer=opt)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "OX_OhXE1046u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#start training\n",
        "\n",
        "model.fit(train_univariate, epochs=epochs,\n",
        "                      steps_per_epoch=500,\n",
        "                      validation_data=val_univariate, validation_steps=50)"
      ],
      "metadata": {
        "id": "kkgjNh3g06cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot prediction v.s true values\n",
        "pred_y=model.predict(x_val)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(y_val[0:300],label=\"true\")\n",
        "plt.plot(pred_y[0:300],label=\"pred\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g-HsHQDD07yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Einfaches Beispiel LSTM"
      ],
      "metadata": {
        "id": "K1XOOxyS0_ZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#build LSTM model by means of keras\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "\n",
        "#model.add(layers.GRU(20,activation='tanh',input_shape=x_train.shape[-2:])) #A_1 in upper diagram\n",
        "model.add(layers.LSTM(20,activation='tanh',input_shape=x_train.shape[-2:])) #A_1 in upper diagram\n",
        "model.add(layers.Dense(1,activation='tanh')) #A_2 in the upper diagram\n",
        "\n",
        "# setup optimizer and define loss\n",
        "opt=optimizers.Adam(learning_rate=0.0005)\n",
        "model.compile(loss='mae', optimizer=opt)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "Sew6iJ1s1CKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#start training\n",
        "\n",
        "model.fit(train_univariate, epochs=epochs,\n",
        "                      steps_per_epoch=500,\n",
        "                      validation_data=val_univariate, validation_steps=50)"
      ],
      "metadata": {
        "id": "JYzwYLZQ1Dvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot prediction v.s true values\n",
        "pred_y=model.predict(x_val)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(y_val[0:600],label=\"true\")\n",
        "plt.plot(pred_y[0:600],label=\"pred\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cXdo5qir1FCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example classification of movie reviews"
      ],
      "metadata": {
        "id": "OSYBVt0Z1Klx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "\n",
        "dataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True,\n",
        "                          as_supervised=True)\n",
        "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
        "\n",
        "encoder = info.features['text'].encoder\n",
        "\n",
        "print ('Vocabulary size: {}'.format(encoder.vocab_size))"
      ],
      "metadata": {
        "id": "A2pIj31m1LNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test word embedding attached to the data set\n",
        "\n",
        "sample_string = 'Hallo Saarland'\n",
        "\n",
        "encoded_string = encoder.encode(sample_string)\n",
        "print (encoded_string)\n",
        "\n",
        "original_string = encoder.decode(encoded_string)\n",
        "print (original_string)"
      ],
      "metadata": {
        "id": "oQ7edjHA1NSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare dataset\n",
        "buffer_size = 10000\n",
        "batch_size = 64\n",
        "\n",
        "train_dataset = train_dataset.shuffle(buffer_size)\n",
        "# train_dataset = train_dataset.padded_batch(batch_size, train_dataset.output_shapes)\n",
        "train_dataset = train_dataset.padded_batch(batch_size, tf.compat.v1.data.get_output_shapes(train_dataset))\n",
        "\n",
        "# test_dataset  = test_dataset.padded_batch(batch_size, test_dataset.output_shapes)\n",
        "test_dataset  = test_dataset.padded_batch(batch_size, tf.compat.v1.data.get_output_shapes(test_dataset))"
      ],
      "metadata": {
        "id": "xndM4u181Pf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build LSTM Model\n",
        "lstm_model = tf.keras.Sequential()\n",
        "\n",
        "lstm_model.add(tf.keras.layers.Embedding(encoder.vocab_size, 64))\n",
        "lstm_model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))\n",
        "lstm_model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "lstm_model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "\n",
        "#setup training\n",
        "lstm_model.compile(loss='binary_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=['accuracy'])\n",
        "lstm_model.summary()\n",
        "\n",
        "#start training\n",
        "lstm_model.fit(train_dataset, epochs=1,\n",
        "                    validation_data=test_dataset,\n",
        "                    validation_steps=30)\n"
      ],
      "metadata": {
        "id": "CBsRUyZu1Qqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test the model\n",
        "\n",
        "input_text='This is a nice movie'\n",
        "encoded_text=tf.expand_dims(tf.cast(encoder.encode(input_text),tf.float32),0)\n",
        "print(encoded_text)\n",
        "print(lstm_model.predict(encoded_text))"
      ],
      "metadata": {
        "id": "Ay8ggF7S1SKm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}